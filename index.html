<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>
    Arousal Valence Model-based Logit Steering for Emotion-Controllable Song Generation
  </title>

  <style>
    body {
      font-family: Arial, Helvetica, sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px;
      line-height: 1.6;
      color: #111;
    }

    h1 {
      text-align: center;
      font-size: 32px;
      margin-bottom: 30px;
    }

    h2 {
      font-size: 22px;
      margin-top: 40px;
      border-bottom: 1px solid #ddd;
      padding-bottom: 6px;
    }

    p {
      font-size: 16px;
      text-align: justify;
    }

    img {
      display: block;
      margin: 30px auto;
      max-width: 100%;
    }

    .abstract {
      background-color: #f8f8f8;
      padding: 20px;
      border-radius: 6px;
    }
  </style>
</head>

<body>

<h1>
  Arousal Valence Model-based Logit Steering for Emotion-Controllable Song Generation
</h1>

<h2>Abstract</h2>
<div class="abstract">
  <p>
    The task of emotion-controllable song generation aims to produce songs that
    align with given lyrics and a target emotion. Existing methods typically rely
    on discrete emotion labels or text-based prompts, which often lead to unstable
    and imprecise emotional control. To address these limitations, we propose ECSG,
    a continuous emotion control framework based on the Arousal–Valence model,
    enabling stable and accurate emotion manipulation in a training-free manner.
  </p>

  <p>
    To realize such stable and precise emotional control, we introduce an Arousal
    Valence-based logit steering mechanism that directly regulates the decoding
    dynamics of the generator. At each generation step, the model evaluates the
    top-k candidate tokens by predicting their corresponding arousal–valence
    coordinates and computing their similarity to the target emotion. These
    emotion-alignment scores are then fused with the raw logits to bias the
    sampling distribution toward candidates that better follow the desired
    emotional trajectory. Experimental results demonstrate that ECSG achieves
    significant improvements in emotion deviation, audio quality, and structural
    coherence compared with existing models, and exhibits superior emotional
    expressiveness and overall musicality in subjective evaluations.
  </p>
</div>

<h2>Model Overview</h2>
<img src="images/model.png" alt="Model architecture of ECSG">

</body>
</html>
